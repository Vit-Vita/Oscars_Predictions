{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3e2bf8-7a26-4eca-9a02-8e8984d8b998",
   "metadata": {},
   "source": [
    "# Group 07 - Code - Rotten Tomatoes Web Scraping\n",
    "## Vedenikova_Vitalia_StudentB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c896c-5c97-4339-b892-59f7849534cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from requests_html import HTMLSession\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from ast import literal_eval\n",
    "pd.set_option('display.max_colwidth', None) #allows to see all the columns when working with data frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5f4ae-9ab1-4dd9-89aa-a50ea01b28a6",
   "metadata": {},
   "source": [
    "## 2. URLs to access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d348e0-6ae8-4056-93ba-bc438dabb286",
   "metadata": {},
   "source": [
    "### 2.1 Generating the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324b4e0-b611-4e66-95f3-eba958040cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Student A's data frame\n",
    "dfA= pd.read_csv(\"../Code/Data/StudentA_Early_Stage_1.csv\")\n",
    "dfA = dfA.rename(columns={'Movie Title':'Movie_Title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcf799-3fee-41c1-a810-f9094f313a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Student C's data frame and merge it with Student A's \n",
    "dfC= pd.read_csv(\"../Code/Data/StudentC_Early_Stage_1.csv\")\n",
    "dfC = dfC.rename(columns={'Title':'Movie_Title', 'Year':'Release_Year'})\n",
    "df = pd.merge(dfA, dfC, on='Movie_Title', how='right')\n",
    "\n",
    "#Keep only the useful columns\n",
    "df=df.drop(df.loc[:, 'Released':'BoxOffice'].columns, axis=1)\n",
    "df=df.drop(['Unnamed: 0_x','Oscar Ceremony Year','Unnamed: 0_y'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2646f-fa72-45bf-868f-b78002387ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the URLs that we will be accessing\n",
    "\n",
    "def generate_movie_links(df) -> None:\n",
    "    \"\"\"function to add columns to the dataframe, reformat the movie names and create an URL for each movie in the dataframe\"\"\"\n",
    "    df.insert(2,\"URL_Movie_Names\", True)\n",
    "    df.insert(3,\"Rottentomatoes_URLs\", True)\n",
    "    df['URL_Movie_Names'] = df['Movie_Title'].str.lower() \n",
    "    df['URL_Movie_Names'] = df['URL_Movie_Names'].str.split('(').str[0] \n",
    "    df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(' ', '_')\n",
    "    df['URL_Movie_Names'] = df['URL_Movie_Names'].str.rstrip('_') \n",
    "\n",
    "\n",
    "    Char_replacements = [('&', 'and'),('é', 'e'),('á', 'a'),('à', 'a'),('í', 'i'),('ì', 'i'),('ǎ', 'a'),('ó', 'o'),('...', '_'),\n",
    "                          ('_-_', '_')]\n",
    "\n",
    "    for i,j in Char_replacements:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(i, j)\n",
    "\n",
    "    \n",
    "    URL_replacements = [('13_hours', '13_hours_the_secret_soldiers_of_benghazi'),('birdman_or', 'birdman'),('spider-man', 'spider_man'),\n",
    "                        ('spider-verse', 'spider_verse'),('maggie_simpson_in', 'the_simpsons'),('ennemis_interieurs', 'enemies_within'),\n",
    "                        ('everything_will_be_okay', 'grill_gut_alles_gut'),('the_white_tiger', 'the_white_tiger_2020'),\n",
    "                        ('half-blood', 'half_blood'),('the_wife', 'the_wife_2018'),('lifeboat', 'lifeboat_2019'),\n",
    "                        ('kung_fu_panda_2', 'kung_fu_panda_the_kaboom_of_doom'),('les_mis_rables', 'les_miserables'),\n",
    "                        ('star_wars_episode_viii_the_last_jedi', 'star_wars_the_last_jedi'),('star_wars_episode_ix_the_rise_of_skywalker', 'star_wars_the_rise_of_skywalker'),\n",
    "                        ('music_by_prudence', 'music_by_prudence_2011'),('once_upon_a_time__in', 'once_upon_a_time_in'),\n",
    "                        ('the_baader_meinhof_complex', 'baader_meinhof_complex'),('the_curious_case_of_benjamin_button', 'curious_case_of_benjamin_button'),\n",
    "                        ('the_last_station', 'last_station'),('golda', 'golda_2022'),('the_milk_of_sorrow', 'milk_of_sorrow'),('the_young_victoria', 'young_victoria'),\n",
    "                        ('undefeated', 'undefeated_2012'),('one_night_in_miami_', 'one_night_in_miami'),('ala_kachuu', 'ala_kachuu_take_and_run'),('ave_maria', '771428498'),\n",
    "                        ('changeling', '1191742_changeling'),('glass_onion', 'glass_onion_a_knives_out_mystery'),('joanna', '771467811'),('hellboy_ii', 'hellboy_2'),\n",
    "                        ('the_100_yearold', 'the_100_year_old'),('the_betrayal', 'nerakhoon'),('the_present', 'the_present_0000'),('the_reader', 'reader'),\n",
    "                        ('the_secret_in_their_eyes', 'secret_in_their_eyes'),('tick_tick__boom', 'tick_tick_boom')]\n",
    "                        \n",
    "    for i,j in URL_replacements:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(i, j)\n",
    "\n",
    "\n",
    "\n",
    "    lsch = \"!?.’:',+/-\\\"\" \n",
    "    for ch in lsch:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(ch, '')\n",
    "    \n",
    "  \n",
    "    Additional_replacements = [('chinas_unnatural_disaster_the_tears_of_sichuan_province', 'chinas-unnatural-disaster-the-tears-of-sichuan-province'),\n",
    "                               ('encounters_at_the_end_of_the_world', '1194818-encounters_at_the_end_of_the_world'),('fantastic_mr_fox', '1197696-fantastic_mr_fox'),\n",
    "                               ('how_they_donut_a_matter_of_loaf_and_death', 'wallace_and_gromit_a_matter_of_loaf_and_death_pands'),\n",
    "                               ('lavatory_lovestory', 'ubornaya-istoriya-lyubovnaya-istoriya'),('pitääkö_mun_kaikki_hoitaa', 'pitaako_mun_kaikki_hoitaa'),\n",
    "                               ('smile_pinki', 'smile-pinki'),('the_100yearold_man', 'the_100_year_old_man'),('the_conscience_of_nhem_en', 'the-conscience-of-nhem-en'),\n",
    "                               ('the_duchess', '10009493-duchess'),('the_final_inch', 'the-final-inch'),('the_last_truck_closing_of_a_gm_plant', 'the-last-truck-the-closing-of-a-gm-plant'),\n",
    "                               ('the_lovely_bones', '1189344-lovely_bones'),('the_wolfman', '1194949-wolfman'),('tron_legacy', '10011582-tron_legacy'),\n",
    "                               ('winters_bone', '10012136-winters_bone'),('wreckit_ralph', 'wreck_it_ralph'),('xmen_days_of_future_past', 'x_men_days_of_future_past'),\n",
    "                               ('yespeople', 'ja_folkid'),('dimanchesunday', 'dimanche'),('ninetyfive_senses', 'ninety_five_senses'),\n",
    "                               ('the_most_dangerous_man_in_america_daniel_ellsberg_and_the_pentagon_papers', 'daniel_ellsberg_doc'),\n",
    "                               ('precious_based_on_the_novel_push_by_sapphire', 'precious'),('shangchi', 'shang_chi'),\n",
    "                               ('star_wars_the_force_awakens', 'star_wars_episode_vii_the_force_awakens'),('the_cove', '1208882-cove'),\n",
    "                               ('wall·e', 'wall_e'),\n",
    "                               ('borat_subsequent_moviefilm_delivery_of_prodigious_bribe_to_american_regime_for_make_benefit_once_glorious_nation_of_kazakhstan', 'borat_subsequent_moviefilm'),\n",
    "                               ('alice_in_wonderland', '1221547-alice_in_wonderland'),('a_single_man', '1218217-single_man'),\n",
    "                               ('burma_vj_reporting_from_a_closed_country', 'burma-vj'),('departures', '10010675-departures'),\n",
    "                               ('kontiki', 'kon_tiki'),('the_house_of_small_cubes', 'la-maison-en-petits-cubes'),\n",
    "                               ('the_visitor', '10008820-visitor'),('wanted', '1174279-wanted'),\n",
    "                               ('watu_wote_all_of_us', 'watu_wote'),('happygolucky', '1195103-happy_go_lucky'),\n",
    "                               ('salt', '1213717-salt')]\n",
    "    \n",
    "    for i,j in Additional_replacements:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(i, j)\n",
    "    \n",
    "    #create the URLs\n",
    "    df['Rottentomatoes_URLs'] = [ f'https://www.rottentomatoes.com/m/{x}' for x in df['URL_Movie_Names']] #create the URLs\n",
    "    \n",
    "generate_movie_links(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f29af3-d765-4cb2-8762-f11e039fab7b",
   "metadata": {},
   "source": [
    "Here below we see the sort of result we can expect from running the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373971b-7d47-46e1-ba59-29502c7f8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Movie_Title'] == 'Changeling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fd883-6728-4381-aad2-14c88a848106",
   "metadata": {},
   "source": [
    "### 2.2 Fixing the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22d673-9c5a-47e2-b5cb-457624edbf4e",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** : As of 18.04.24 the below will no longer work with the URLs we previously defined since Rotten Tomatoes updated the movies' pages. \n",
    "As a consequence, we will be using a handful of links gathered from https://web.archive.org/ to demonstrate how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be68eb9-64ef-4791-a45a-a2b528c01906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../Code/Data/df_waybackmachine_links.csv\") # <---- this is the csv we will be using to show the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dfbe1-407f-4b4e-85f3-80130553220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fix the URLs by using a separate function\n",
    "\n",
    "myheaders = {'User-Agent' : 'Mozilla/5.0 (X11; Linux x86_64) \\ '\n",
    "              'AppleWebKit/537.36 (KHTML, like Gecko) \\ ' \t\t        \n",
    "              'Chrome/84.0.4147.89 Safari/537.36'}\n",
    "\n",
    "headers = {'User-Agent': myheaders}\n",
    "\n",
    "\n",
    "def fix_urls(df):\n",
    "    \"\"\"function that fixes defective URLs in the dataframe by adding the release year at the end of the link and separating the links\n",
    "    still defective for further investigation. There are two situations in which we add the year: 1) the link itself is defective since\n",
    "    we retrieve nothing for movie data or 2)the release year on the movie page does not match the official release year of the film\"\"\"\n",
    "    \n",
    "    for row_index, row in df.iterrows(): \n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        page_text = requests.get(link, headers=myheaders).text\n",
    "        soup = BeautifulSoup(page_text,'lxml')\n",
    "        movie_characteristics = soup.find('div', class_= \"panel-body content_body\") \n",
    "        movie_release_year= soup.find('p', class_= \"info\", attrs={'data-qa':'score-panel-subtitle'})\n",
    "        \n",
    "        try:\n",
    "            if(movie_characteristics == None):\n",
    "                new_link = link+'_'+ str(row['Release_Year']) \n",
    "                df.loc[row_index,'Rottentomatoes_URLs'] =  new_link \n",
    "                page_text = requests.get(new_link, headers=myheaders).text\n",
    "                soup = BeautifulSoup(page_text,'lxml')\n",
    "                movie_characteristics = soup.find('div', class_= \"panel-body content_body\")\n",
    "                if (movie_characteristics == None): \n",
    "                    with open('links_to_fix.csv', 'a', newline='') as file:\n",
    "                        file.write(f\"{df.loc[row_index,'Movie_Title']}, {new_link},\\n\")\n",
    "                    continue\n",
    "            elif(int(movie_release_year.text[0:4]) != int(row['Release_Year'])):\n",
    "                new_link = link+'_'+ str(row['Release_Year']) \n",
    "                df.loc[row_index,'Rottentomatoes_URLs'] =  new_link \n",
    "                page_text = requests.get(new_link, headers=myheaders).text\n",
    "                soup = BeautifulSoup(page_text,'lxml')\n",
    "                movie_characteristics = soup.find('div', class_= \"panel-body content_body\")\n",
    "                if (movie_characteristics == None): \n",
    "                    with open('links_to_fix.csv', 'a', newline='') as file:\n",
    "                        file.write(f\"{df.loc[row_index,'Movie_Title']}, {new_link},\\n\") \n",
    "                    continue\n",
    "        except ValueError: \n",
    "            new_link = link+'_'+ str(row['Release_Year']) \n",
    "            df.loc[row_index,'Rottentomatoes_URLs'] =  new_link \n",
    "            page_text = requests.get(new_link, headers=myheaders).text\n",
    "            soup = BeautifulSoup(page_text,'lxml')\n",
    "            movie_characteristics = soup.find('div', class_= \"panel-body content_body\")\n",
    "            if (movie_characteristics == None): \n",
    "                with open('links_to_fix.csv', 'a', newline='') as file:\n",
    "                    file.write(f\"{df.loc[row_index,'Movie_Title']}, {new_link},\\n\") \n",
    "                continue\n",
    "\n",
    "fix_urls(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d42ed9-6a87-4d2d-b747-69874285259a",
   "metadata": {},
   "source": [
    "Go back to the link generator function for any adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6b786-259a-474a-a76f-274126d6a244",
   "metadata": {},
   "source": [
    "Here below you can see the result.\n",
    "I made **two links purposefully erronous** to demonstrate that the function works properly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a1ced-0b4a-48d1-90c6-7f377dcf96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15fa8c-ec99-4104-a2d3-c77c398bb28d",
   "metadata": {},
   "source": [
    "### 2.3 Removing some movies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01667258-552f-4e79-a9a4-11aa6c363553",
   "metadata": {},
   "source": [
    "10 movies were not found on the website and are removed from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78524609-8990-42e9-b6ae-cbeaf074c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 rows are removed from the dataframe\n",
    "df_fixed = pd.read_csv(\"../Code/Data/df_with_most_links_fixed.csv\")\n",
    "df_fixed = df_fixed.drop('Unnamed: 0', axis=1)\n",
    "df_links = pd.read_csv(\"../Code/Data/links_to_fix_original.csv\", header=None)\n",
    "df_links.columns =['Movie_Title', 'Rottentomatoes_URLs']\n",
    "df = pd.merge(df_links, df_fixed, on=['Movie_Title'], how='outer', indicator=True).query(\"_merge != 'both'\").drop('_merge', axis=1).reset_index(drop=True)\n",
    "df = df.drop('Rottentomatoes_URLs_x', axis=1)\n",
    "df = df.rename(columns={'Rottentomatoes_URLs_y': 'Rottentomatoes_URLs'})\n",
    "df.to_csv('../Code/Data/df_links_all_ok_new.csv', sep=',',index=True, encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf768572-d64d-47c3-884d-37218f292148",
   "metadata": {},
   "source": [
    "## 3. Scraping Basic Information About The Movies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b44a4-4c95-496a-a2cc-acc73c5ddd3a",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** : As of 18.04.24 the below will no longer work with the URLs we previously defined since Rotten Tomatoes updated the movies' pages. \n",
    "As a consequence, we will be using a handful of links gathered from https://web.archive.org/ to demonstrate how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2706203d-ac26-42d6-b098-cb92445c12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= pd.read_csv(\"../Code/Data/df_links_all_ok.csv\")  #<--- this is our data set with all the Oscar movies and their links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3465faf-fbb3-4b1d-9a66-a7c8375a9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../Code/Data/df_waybackmachine_links.csv\") # <---- this is the csv we will be using to show the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e050e8-461a-4dc8-9c40-258db9614141",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_characteristics = ['Rating','Genre', 'Original Language', 'Director', 'Producer','Cast and Crew',\n",
    "                         'Writer', 'Release Date (Theaters)', 'Release Date (Streaming)', 'Rerelease Date (Theaters)', 'Box Office (Gross USA)',\n",
    "                         'Runtime', 'Distributor','Production Co', 'Sound Mix', 'Aspect Ratio', 'Synopsis', 'Tomatometer', 'Tomatometer State', \n",
    "                         'Audience Score', 'Critics Concensus']\n",
    "new_vals = [None for item in basic_characteristics]\n",
    "new_col = dict(zip(basic_characteristics, new_vals))\n",
    "df = df.assign(**new_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23671a86-4d46-4989-99e9-af1e6c022b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "myheaders = {'User-Agent' : 'Mozilla/5.0 (X11; Linux x86_64) \\ '\n",
    "              'AppleWebKit/537.36 (KHTML, like Gecko) \\ ' \t\t        \n",
    "              'Chrome/84.0.4147.89 Safari/537.36'}\n",
    "\n",
    "headers = {'User-Agent': myheaders}\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None) \n",
    "\n",
    "#Scraping each movie page's information using Beautiful Soup and Requests\n",
    "def scrape_page_information (df) -> pd.DataFrame:\n",
    "    \"\"\"function to scrape the information on the movie's page and place it in a dataframe\"\"\"\n",
    "    \n",
    "    not_crew_members_list =[]\n",
    "    \n",
    "    for row_index, row in df.iterrows():  \n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        page_text = requests.get(link, headers=myheaders).text \n",
    "        soup = BeautifulSoup(page_text,'lxml')\n",
    "        \n",
    "        #We look for the most basic information first (director, producer, runtime, etc.)\n",
    "        try:\n",
    "            movie_characteristics = soup.find('div', class_= \"panel-body content_body\") # in this class we retrieve the basic information\n",
    "            movie_item_labels = list(movie_characteristics.find_all('b', attrs={\"data-qa\":\"movie-info-item-label\"}))\n",
    "            movie_item_values = movie_characteristics.find_all('span', attrs={\"data-qa\":\"movie-info-item-value\"})\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        parsed_data = {}\n",
    "        for idx,label in enumerate(movie_item_labels):\n",
    "            label = label.text.rstrip(':')\n",
    "            lsch = ['\\n', '  '] \n",
    "            parsed_data[label] = movie_item_values[idx].text\n",
    "            for ch in lsch:\n",
    "                parsed_data[label] = parsed_data[label].replace(ch, '')            \n",
    "            df.loc[row_index, label] = parsed_data[label] \n",
    "        \n",
    "        #We look for the audiencescore, tomatometerscore, tomatometerstate: \n",
    "        attrs = soup.find('score-board-deprecated').attrs               \n",
    "        for attr in ['audiencescore', 'tomatometerscore', 'tomatometerstate']:\n",
    "            df.loc[row_index, 'Tomatometer'] = attrs['tomatometerscore']\n",
    "            df.loc[row_index, 'Tomatometer State'] = attrs['tomatometerstate']\n",
    "            df.loc[row_index, 'Audience Score'] = attrs['audiencescore']\n",
    "\n",
    "        #We look for the critics consensus \n",
    "        consensus = soup.find('span', attrs={'data-qa':'critics-consensus'}) \n",
    "        try:\n",
    "            consensus = consensus.text\n",
    "            df.loc[row_index, 'Critics Concensus'] = consensus\n",
    "        except AttributeError:\n",
    "            df.loc[row_index, 'Critics Concensus'] = 'NaN'\n",
    "            \n",
    "        #We look for the synopsis of the movie   \n",
    "        synopsis = soup.find('p', attrs={'data-qa':'movie-info-synopsis'})\n",
    "        try:\n",
    "            synopsis = synopsis.text\n",
    "            df.loc[row_index, 'Synopsis'] = synopsis\n",
    "        except AttributeError:\n",
    "            df.loc[row_index, 'Synopsis'] = 'NaN'\n",
    "\n",
    "        #We look for all the crew members and their roles/jobs in the movie production \n",
    "        crew = soup.css.select('a > p', class_='p--small', attrs={'data-qa':'cast-crew-item-link', 'id':'cast-and-crew', 'href':True})\n",
    "        role = soup.find_all('p', class_='p--small')\n",
    "        \n",
    "        roles_list=[]\n",
    "        for r in role:\n",
    "            roles_list.append(r.get_text(strip=True))\n",
    "               \n",
    "        crew_list=[]\n",
    "        for c in crew[1:]:\n",
    "            crew_list.append(c.get_text(strip=True))\n",
    "\n",
    "        not_crew_members = crew_list[len(roles_list):]#We append this to 'not_crew_members_list' created for the cleaning section \n",
    "        for n in not_crew_members:\n",
    "            not_crew_members_list.append(n)\n",
    "        \n",
    "        crew_list = crew_list[:len(roles_list)] \n",
    "       \n",
    "        tuples = zip(crew_list, roles_list)  \n",
    "        full_crew = [f'{pair[0]} ({pair[1]})' for pair in tuples] \n",
    "        full_crew = ', '.join(map(str, full_crew))\n",
    "        \n",
    "        df['Cast and Crew'] = df['Cast and Crew'].astype('object')\n",
    "        df.at[row_index, 'Cast and Crew'] =full_crew \n",
    "        \n",
    "    return df, not_crew_members_list\n",
    "\n",
    "\n",
    "df, not_crew_members_list = scrape_page_information(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0f2c8-c883-45ea-b165-0fdbd7244d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d846550-f39a-42df-9c71-1f24253128d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid running the function again, we pickle the set of our not_crew_members_list:\n",
    "\n",
    "not_crew_members_list_set = list(set(not_crew_members_list))\n",
    "\n",
    "with open('not_crew_pickle_extra.pkl', 'wb') as f:\n",
    "    pickle.dump(not_crew_members_list_set,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe13ef9-51bb-4ab3-940a-59347120e8cf",
   "metadata": {},
   "source": [
    "## 4. Scraping Critics Reviews of the Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94723bb-55fd-49c3-9264-50b365eedb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv(\"../Code/Data/df_links_all_ok.csv\")\n",
    "df_links = df_links.drop('Unnamed: 0', axis=1)\n",
    "df_links.insert(1, 'Reviews_URLs', True)\n",
    "for row_index, row in df_links.iterrows():\n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        df_links.loc[row_index, 'Reviews_URLs'] = link+'/reviews?intcmp=rt-scorecard_tomatometer-reviews'\n",
    "links = df_links['Reviews_URLs']\n",
    "df_links = pd.DataFrame(links)\n",
    "df_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffc679-cc53-4ce5-9dc6-0fc53c8bff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping all of the critics' comments using Selenium \n",
    "\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "myheaders = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "chrome_options.add_argument(f\"user-agent={myheaders}\")\n",
    "#chrome_options.add_argument('--headless=new')  \n",
    "headers = {'User-Agent': myheaders}\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def scrape_critics_reviews (df_links, driver) -> pd.DataFrame:\n",
    "    \"\"\"function to scrape all the critics reviews by using Selenium and placing them in data frames which we then concatenate into a single\n",
    "    dataframe\"\"\"\n",
    "    \n",
    "    data_frames_list=[] #this is where we store the data frames\n",
    "    \n",
    "    for row_index, row in df_links.iterrows():\n",
    "        link = row['Reviews_URLs']\n",
    "        driver.get(link)\n",
    "        \n",
    "        #keep clicking the 'load more' button as long as it exists to get all the comments \n",
    "        while True:\n",
    "            time.sleep(1.5)\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"reviews\"]/div[2]/rt-button').click()\n",
    "            except (NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException):  \n",
    "                break \n",
    "\n",
    "        #scrape comments\n",
    "        reviews = driver.find_elements(By.CLASS_NAME,'review-row')\n",
    "        reviews_list = []\n",
    "    \n",
    "        for review in reviews:\n",
    "            try:\n",
    "                name_critic = review.find_element(By.CSS_SELECTOR,'a.display-name').text \n",
    "                name_journal = review.find_element(By.CSS_SELECTOR,'a.publication').text\n",
    "                comment = review.find_element(By.CSS_SELECTOR,'p.review-text').text\n",
    "                positive_or_negative = review.find_element(By.CSS_SELECTOR,'score-icon-critic-deprecated[alignment=left]').get_attribute('state')\n",
    "                date = review.find_element(By.CSS_SELECTOR,'span[data-qa=review-date]').text\n",
    "            except (NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException):\n",
    "                continue\n",
    "            \n",
    "            reviews_list.append({\n",
    "                'Date':date, 'Author':name_critic, 'Journal':name_journal, 'Positive_Negative':positive_or_negative, 'Reviews':comment})\n",
    "            \n",
    "            df_temp = pd.DataFrame(reviews_list)\n",
    "            df_temp.insert(1, 'Reviews_URLs', row['Reviews_URLs'])\n",
    "            \n",
    "         \n",
    "        data_frames_list.append(df_temp)\n",
    "\n",
    "    driver.quit() \n",
    "    df_result = pd.concat(data_frames_list, ignore_index=True) \n",
    "    return df_result\n",
    "\n",
    "df_result = scrape_critics_reviews(df_links, driver)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a08cec-15b5-42be-a983-c52c173df890",
   "metadata": {},
   "source": [
    "## 5. Merge the Datasets - Movie Characterisitcs and Critics' Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54560c00-1deb-4e7b-b155-0ff902ee4021",
   "metadata": {},
   "source": [
    "Finally, we merge the datasets for the movie page information and the dataset for the reviews. The result is the stage 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1cec3f-0024-44df-8354-b6cd3c73c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.read_csv(\"../Code/Data/data_all_reviews.csv\")\n",
    "df_moviedata = pd.read_csv(\"../Code/Data/movie_page_data.csv\")\n",
    "df_moviedata = df_moviedata.drop(['Unnamed: 0','Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c33e9-1cdb-4ef7-9bd6-105d0c891552",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_characteristics = ['Reviews_URLs']\n",
    "reviews_vals = [None for item in reviews_characteristics]\n",
    "new_col = dict(zip(reviews_characteristics, reviews_vals))\n",
    "df_moviedata = df_moviedata.assign(**new_col)\n",
    "for row_index, row in df_moviedata.iterrows():\n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        df_moviedata.loc[row_index, 'Reviews_URLs'] = link+'/reviews?intcmp=rt-scorecard_tomatometer-reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9f5a5-ebcd-463c-9320-1e162977f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merged = pd.merge(df_moviedata, df_result, on='Reviews_URLs', how='left')\n",
    "df_merged.to_csv('Vedenikova_Vitalia_studentB_stage1.csv', sep=',',index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c27e41-e3a3-490f-bff2-fa12cebec27b",
   "metadata": {},
   "source": [
    "## 6. Clean the Resulting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59708f-0506-4e7c-8011-2017662d3ec6",
   "metadata": {},
   "source": [
    "### 6.1 Encode the CSV file correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd2f5e-fd62-499b-bc77-d8209c167835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Vedenikova_Vitalia_studentB_stage1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b219d8-898f-4337-85cd-9da1f6a1be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vedenikova_Vitalia_studentB_stage3.csv', sep=',',index=False, encoding='utf-8-sig')\n",
    "df = pd.read_csv('Vedenikova_Vitalia_studentB_stage3.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2905e25-66b2-443d-9663-f718989b1ed7",
   "metadata": {},
   "source": [
    "### 6.2 Remove Columns containing unnecessary data and give standardized names to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ab4cc-f9f6-4821-be4e-12483b7ec7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0','Unnamed: 0.1','URL_Movie_Names', 'Reviews_URLs', 'Release_Year',\n",
    "              'Rerelease Date (Theaters)', 'Release Date (Streaming)', 'Release Date (DVD)','Sound Mix', 'Aspect Ratio', \n",
    "              'View the collection'], axis=1)\n",
    "\n",
    "df = df.rename(columns={'Date':'Review_Date', 'Original Language':'Original_Language', 'Cast and Crew': 'Cast_And_Crew', \n",
    "                        'Release Date (Theaters)': 'Release_Date_(Theaters)', 'Box Office (Gross USA)':'Box_Office_(Gross_USA)', \n",
    "                        'Production Co':'Production_Co','Tomatometer State':'Tomatometer_State', \n",
    "                        'Audience Score':'Audience_Score','Critics Concensus':'Critics_Concensus'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e665b8-a8de-4f3f-bc3a-2bc12609f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf55c96-cbd4-448e-9776-e215882cd6e9",
   "metadata": {},
   "source": [
    "### 6.3 Remove the extra spaces (such as \\n, \\s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe4ab3-9dcc-4119-84b3-e1795b5321da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cast_And_Crew'][2594]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0106c6-8e76-4a9f-ab96-fd4e2e1cee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_list = ['Synopsis','Cast_And_Crew']\n",
    "df[col_list] = df[col_list].apply(lambda x : x.str.strip())\n",
    "\n",
    "\n",
    "df = df.replace(r'\\n{1,}\\s{2,}', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f318cc-7f8c-49e6-b413-c3c97c829562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result:\n",
    "df['Cast_And_Crew'][2594]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb2e4e-faa1-4659-9fa0-f7c520c08581",
   "metadata": {},
   "source": [
    "### 6.4 Remove Unnecessary Characters and Inaccurate Information for the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6e91f-f711-4e4a-aa66-86a69c43f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Release_Date_(Theaters)'] = df['Release_Date_(Theaters)'].str[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810bd2d-a487-40ad-a9b5-2ccbfa7dcc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#We deserialize our variable:\n",
    "with open('../Code/Data/not_crew_pickle_extra.pkl', 'rb') as f:\n",
    "    not_crew_list = pickle.load(f) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54b33d-d819-42ae-ae4e-7d667aa995c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Cast_And_Crew'][6915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be474c9c-ae39-4e4b-9242-19d910e1dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row_index, row in df.iterrows():\n",
    "    for element in not_crew_list:\n",
    "        try: \n",
    "            if element in row['Cast_And_Crew']:\n",
    "                text= row['Cast_And_Crew']\n",
    "                t = re.sub(rf'{element}.*\\)','',text)[0:-2]  \n",
    "                df.loc[row_index, 'Cast_And_Crew'] = t\n",
    "        except TypeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f59173-4356-46cf-98cf-62ee7a916aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n",
    "df['Cast_And_Crew'][6915]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9cc1c7-227c-447d-92c8-67c48357533c",
   "metadata": {},
   "source": [
    "### 6.5 Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ab61e-88ce-4902-888b-3e9d34391e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Cast_And_Crew'] = df['Cast_And_Crew'].str.replace('()', '(NaN)')\n",
    "\n",
    "\n",
    "df = df.fillna(\"NaN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e907c-94c9-4349-9643-462b8ff66b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "review_counts = df[\"Movie_Title\"].value_counts().sort_index()\n",
    "review_counts = review_counts.to_frame()\n",
    "\n",
    "subcategory = review_counts[review_counts['count']>1]\n",
    "subcategory = subcategory.reset_index()\n",
    "subcategory_list = [movie for movie in subcategory['Movie_Title']]\n",
    "\n",
    "\n",
    "for row_index, row in df.iterrows():\n",
    "    if df.at[row_index,'Reviews'] == 'NaN' and row['Movie_Title'] in subcategory_list:\n",
    "        df.drop(row_index,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baee8de-f4bc-42b5-8cff-b3d634fccf42",
   "metadata": {},
   "source": [
    "### 6.6 Standardize Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a0444-7a82-447b-8738-717700059c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data (df) -> pd.DataFrame:\n",
    "    '''Standardizes the data in the columns 'Release_Date_(Theaters)','Review_Date', 'Runtime' and 'Box_Office_(Gross_USA)' '''\n",
    "    \n",
    "    \n",
    "    df[['Release_Date_(Theaters)','Review_Date']] = df[['Release_Date_(Theaters)','Review_Date']].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "\n",
    "   \n",
    "    time = df['Runtime'].str.extract('(\\d+)h', expand=False).astype(float) * 60 +  df['Runtime'].str.extract('(\\d+)m', expand=False).astype(float)\n",
    "    df['Runtime'] = time\n",
    "\n",
    "    \n",
    "    df['Box_Office_(Gross_USA)'] = df['Box_Office_(Gross_USA)'].str.replace('$', '')\n",
    "    df['Box_Office_(Gross_USA)'] = df['Box_Office_(Gross_USA)'].replace({'K': 'e+03', 'M': 'e+06'}, regex=True).astype(float) \n",
    "    \n",
    "    return df\n",
    "\n",
    "df = standardize_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7637685-6570-4766-a567-91da23ea5fa7",
   "metadata": {},
   "source": [
    "### 6.7 Remove Duplicates\n",
    "Some reviews appear twice and should be removed. The ones with 'NaN' should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311ddf7-d989-4eb7-8da8-6426f9a82e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"NaN\")\n",
    "df = pd.concat([df[df['Reviews']=='NaN'], df[df['Reviews']!='NaN'].drop_duplicates(['Reviews'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ec82e-aa8f-46ba-839b-bc2e825e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = df[\"Reviews\"].value_counts().sort_index()\n",
    "counts = counts.to_frame()\n",
    "reviews_with_duplicates = counts[counts['count']>1]\n",
    "reviews_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d345a-30ee-4cd5-b9b0-9fda10d98414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['Movie_Title','Review_Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f67ad-3610-41c3-94a7-75b971e8a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vedenikova_Vitalia_studentB_stage3.csv', sep=',',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655a970-11c5-42d8-9f76-8a744ab2dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
