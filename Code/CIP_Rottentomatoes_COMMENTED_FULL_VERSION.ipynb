{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3e2bf8-7a26-4eca-9a02-8e8984d8b998",
   "metadata": {},
   "source": [
    "# Group 07 - Code - Rotten Tomatoes Web Scraping\n",
    "## Vedenikova_Vitalia_StudentB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c896c-5c97-4339-b892-59f7849534cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from requests_html import HTMLSession\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from ast import literal_eval\n",
    "pd.set_option('display.max_colwidth', None) #allows to see all the columns when working with data frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb28a6-3f26-4e90-a276-e16d0947027a",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a4e99-6feb-43f0-83f0-88931ea1d399",
   "metadata": {},
   "source": [
    "The **objective** of this scraping process is to scrape the **critics' reviews for films that won the Academy Awards or were nominated from 2009 until 2024** as well as their **characteristics** (director, producer, runtime, cast and crew, etc.) from the website Rotten Tomatoes (URL of the main page : https://www.rottentomatoes.com/) \n",
    "\n",
    "We will ultimately analyse this data to try to better understand what criteria are used to choose an Oscar winner from amongst the nominees.\n",
    "\n",
    "Rotten Tomatoes is a website with frequent updates. It allows for user interactions (anyone can leave a movie review) and its objective is to publish movie ratings which it calculates and constantly updates on the basis of the reviews.  \n",
    "\n",
    "Student A scraped the website of the Academy Awards and produced early stage 1 data containing the names of the movies we will be examining along with the year of the Oscar Ceremony to which they were nominated. Student C also used an API to extract the release dates of the movies along other information and produced an early stage 1 dataset. We will be using some columns of both these data sets to generate the URLs of the movies' pages on Rotten Tomatoes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5f4ae-9ab1-4dd9-89aa-a50ea01b28a6",
   "metadata": {},
   "source": [
    "## 2. URLs to access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d348e0-6ae8-4056-93ba-bc438dabb286",
   "metadata": {},
   "source": [
    "### 2.1 Generating the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5c155-29b0-408a-a0a9-f247f5a8a991",
   "metadata": {},
   "source": [
    "Our first objective is to create a list of URLs for each movie. We use the column with movie names in Student A's data frame and the column with the release dates from Student C's data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324b4e0-b611-4e66-95f3-eba958040cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Student A's data frame\n",
    "dfA= pd.read_csv(\"../Code/Data/StudentA_Early_Stage_1.csv\")\n",
    "dfA = dfA.rename(columns={'Movie Title':'Movie_Title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcf799-3fee-41c1-a810-f9094f313a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Student C's data frame and merge it with Student A's \n",
    "dfC= pd.read_csv(\"../Code/Data/StudentC_Early_Stage_1.csv\")\n",
    "dfC = dfC.rename(columns={'Title':'Movie_Title', 'Year':'Release_Year'})\n",
    "df = pd.merge(dfA, dfC, on='Movie_Title', how='right')\n",
    "\n",
    "#Keep only the useful columns\n",
    "df=df.drop(df.loc[:, 'Released':'BoxOffice'].columns, axis=1)\n",
    "df=df.drop(['Unnamed: 0_x','Oscar Ceremony Year','Unnamed: 0_y'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852ad5c-ffd1-41f2-b974-be859c9267e0",
   "metadata": {},
   "source": [
    "We have to format the URLs while taking the following into consideration:\n",
    "\n",
    "- Usually, the URLs are formatted as follows: https://www.rottentomatoes.com/m/movie_name. Some movies have their year of release at the end of their URL like so: https://www.rottentomatoes.com/m/movie_name_2015. If an error is encountered, the release year has to be added at the end of the URL.\n",
    "- The movie's release year is almost always the year before the Oscars' Ceremony took place. We did discover, however, that this is often not the case, hence the need for Student C to extract release dates from an API.\n",
    "- When there are certain characters such as a dot or apostrophe in the movie name (for example: \"4.1 Miles\"), it is removed in the URL.\n",
    "- If a movie name has parenthesis, the contents of the parenthesis are omitted from the URL link (for example: \"Birdman or (The Unexpected Virtue of Ignorance)\").\n",
    "- Lastly, there are a lot of exceptions to the above rules. Some URLs are formatted differently and have to be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f2646f-fa72-45bf-868f-b78002387ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the URLs that we will be accessing\n",
    "\n",
    "def generate_movie_links(df) -> None:\n",
    "    \"\"\"function to add columns to the dataframe, reformat the movie names and create an URL for each movie in the dataframe\"\"\"\n",
    "    df.insert(2,\"URL_Movie_Names\", True)\n",
    "    df.insert(3,\"Rottentomatoes_URLs\", True)\n",
    "    df['URL_Movie_Names'] = df['Movie_Title'].str.lower() \n",
    "    df['URL_Movie_Names'] = df['URL_Movie_Names'].str.split('(').str[0] #remove any content in parenthesis coming after the title\n",
    "    df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(' ', '_')\n",
    "    df['URL_Movie_Names'] = df['URL_Movie_Names'].str.rstrip('_') #remove any underscore that would be placed at the end of the string\n",
    "\n",
    "    # These characters are generally replaced by different ones\n",
    "    Char_replacements = [('&', 'and'),('é', 'e'),('á', 'a'),('à', 'a'),('í', 'i'),('ì', 'i'),('ǎ', 'a'),('ó', 'o'),('...', '_'),\n",
    "                          ('_-_', '_')]\n",
    "\n",
    "    for i,j in Char_replacements:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(i, j)\n",
    "\n",
    "    #some URLs cannot be fixed systematically and must be fixed separately\n",
    "    \n",
    "    URL_replacements = [('13_hours', '13_hours_the_secret_soldiers_of_benghazi'),('birdman_or', 'birdman'),('spider-man', 'spider_man'),\n",
    "                        ('spider-verse', 'spider_verse'),('maggie_simpson_in', 'the_simpsons'),('ennemis_interieurs', 'enemies_within'),\n",
    "                        ('everything_will_be_okay', 'grill_gut_alles_gut'),('the_white_tiger', 'the_white_tiger_2020'),\n",
    "                        ('half-blood', 'half_blood'),('the_wife', 'the_wife_2018'),('lifeboat', 'lifeboat_2019'),\n",
    "                        ('kung_fu_panda_2', 'kung_fu_panda_the_kaboom_of_doom'),('les_mis_rables', 'les_miserables'),\n",
    "                        ('star_wars_episode_viii_the_last_jedi', 'star_wars_the_last_jedi'),('star_wars_episode_ix_the_rise_of_skywalker', 'star_wars_the_rise_of_skywalker'),\n",
    "                        ('music_by_prudence', 'music_by_prudence_2011'),('once_upon_a_time__in', 'once_upon_a_time_in'),\n",
    "                        ('the_baader_meinhof_complex', 'baader_meinhof_complex'),('the_curious_case_of_benjamin_button', 'curious_case_of_benjamin_button'),\n",
    "                        ('the_last_station', 'last_station'),('golda', 'golda_2022'),('the_milk_of_sorrow', 'milk_of_sorrow'),('the_young_victoria', 'young_victoria'),\n",
    "                        ('undefeated', 'undefeated_2012'),('one_night_in_miami_', 'one_night_in_miami'),('ala_kachuu', 'ala_kachuu_take_and_run'),('ave_maria', '771428498'),\n",
    "                        ('changeling', '1191742_changeling'),('glass_onion', 'glass_onion_a_knives_out_mystery'),('joanna', '771467811'),('hellboy_ii', 'hellboy_2'),\n",
    "                        ('the_100_yearold', 'the_100_year_old'),('the_betrayal', 'nerakhoon'),('the_present', 'the_present_0000'),('the_reader', 'reader'),\n",
    "                        ('the_secret_in_their_eyes', 'secret_in_their_eyes'),('tick_tick__boom', 'tick_tick_boom')]\n",
    "                        \n",
    "    for i,j in URL_replacements:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(i, j)\n",
    "\n",
    "\n",
    "    # These characters generally disappear from Rotten Tomatoes links, so we remove them\n",
    "    lsch = \"!?.’:',+/-\\\"\" #characters to remove in the URL\n",
    "    for ch in lsch:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(ch, '')\n",
    "    \n",
    "    #some additional separate adjustments (urls that must have hyphens for example)\n",
    "    Additional_replacements = [('chinas_unnatural_disaster_the_tears_of_sichuan_province', 'chinas-unnatural-disaster-the-tears-of-sichuan-province'),\n",
    "                               ('encounters_at_the_end_of_the_world', '1194818-encounters_at_the_end_of_the_world'),('fantastic_mr_fox', '1197696-fantastic_mr_fox'),\n",
    "                               ('how_they_donut_a_matter_of_loaf_and_death', 'wallace_and_gromit_a_matter_of_loaf_and_death_pands'),\n",
    "                               ('lavatory_lovestory', 'ubornaya-istoriya-lyubovnaya-istoriya'),('pitääkö_mun_kaikki_hoitaa', 'pitaako_mun_kaikki_hoitaa'),\n",
    "                               ('smile_pinki', 'smile-pinki'),('the_100yearold_man', 'the_100_year_old_man'),('the_conscience_of_nhem_en', 'the-conscience-of-nhem-en'),\n",
    "                               ('the_duchess', '10009493-duchess'),('the_final_inch', 'the-final-inch'),('the_last_truck_closing_of_a_gm_plant', 'the-last-truck-the-closing-of-a-gm-plant'),\n",
    "                               ('the_lovely_bones', '1189344-lovely_bones'),('the_wolfman', '1194949-wolfman'),('tron_legacy', '10011582-tron_legacy'),\n",
    "                               ('winters_bone', '10012136-winters_bone'),('wreckit_ralph', 'wreck_it_ralph'),('xmen_days_of_future_past', 'x_men_days_of_future_past'),\n",
    "                               ('yespeople', 'ja_folkid'),('dimanchesunday', 'dimanche'),('ninetyfive_senses', 'ninety_five_senses'),\n",
    "                               ('the_most_dangerous_man_in_america_daniel_ellsberg_and_the_pentagon_papers', 'daniel_ellsberg_doc'),\n",
    "                               ('precious_based_on_the_novel_push_by_sapphire', 'precious'),('shangchi', 'shang_chi'),\n",
    "                               ('star_wars_the_force_awakens', 'star_wars_episode_vii_the_force_awakens'),('the_cove', '1208882-cove'),\n",
    "                               ('wall·e', 'wall_e'),\n",
    "                               ('borat_subsequent_moviefilm_delivery_of_prodigious_bribe_to_american_regime_for_make_benefit_once_glorious_nation_of_kazakhstan', 'borat_subsequent_moviefilm'),\n",
    "                               ('alice_in_wonderland', '1221547-alice_in_wonderland'),('a_single_man', '1218217-single_man'),\n",
    "                               ('burma_vj_reporting_from_a_closed_country', 'burma-vj'),('departures', '10010675-departures'),\n",
    "                               ('kontiki', 'kon_tiki'),('the_house_of_small_cubes', 'la-maison-en-petits-cubes'),\n",
    "                               ('the_visitor', '10008820-visitor'),('wanted', '1174279-wanted'),\n",
    "                               ('watu_wote_all_of_us', 'watu_wote'),('happygolucky', '1195103-happy_go_lucky'),\n",
    "                               ('salt', '1213717-salt')]\n",
    "    \n",
    "    for i,j in Additional_replacements:\n",
    "        df['URL_Movie_Names'] = df['URL_Movie_Names'].str.replace(i, j)\n",
    "    \n",
    "    #create the URLs\n",
    "    df['Rottentomatoes_URLs'] = [ f'https://www.rottentomatoes.com/m/{x}' for x in df['URL_Movie_Names']] #create the URLs\n",
    "    \n",
    "generate_movie_links(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f29af3-d765-4cb2-8762-f11e039fab7b",
   "metadata": {},
   "source": [
    "Here below we see the sort of result we can expect from running the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373971b-7d47-46e1-ba59-29502c7f8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Movie_Title'] == 'Changeling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc36b4ea-9c25-49ef-91c4-84dd188aedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Movie_Title'] == 'Zootopia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841fd883-6728-4381-aad2-14c88a848106",
   "metadata": {},
   "source": [
    "### 2.2 Fixing the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e844ca-2059-4fa0-9b4a-a1f7d9b09451",
   "metadata": {},
   "source": [
    "We need to check if the URLs we defined work. To do so we loop through the links we generated. If we cannot scrape basic information from the page or if the release year on the page does not match the release year in our dataframe, we append the release year in our dataframe at the end of the URL. If some URLs are still not working at the end of this process, we put them in a separate txt file to be examined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22d673-9c5a-47e2-b5cb-457624edbf4e",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** : As of 18.04.24 the below will no longer work with the URLs we previously defined since Rotten Tomatoes updated the movies' pages. \n",
    "As a consequence, we will be using a handful of links gathered from https://web.archive.org/ to demonstrate how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be68eb9-64ef-4791-a45a-a2b528c01906",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../Code/Data/df_waybackmachine_links.csv\") # <---- this is the csv we will be using to show the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dfbe1-407f-4b4e-85f3-80130553220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fix the URLs by using a separate function\n",
    "\n",
    "myheaders = {'User-Agent' : 'Mozilla/5.0 (X11; Linux x86_64) \\ '\n",
    "              'AppleWebKit/537.36 (KHTML, like Gecko) \\ ' \t\t        \n",
    "              'Chrome/84.0.4147.89 Safari/537.36'}\n",
    "\n",
    "headers = {'User-Agent': myheaders}\n",
    "\n",
    "\n",
    "def fix_urls(df):\n",
    "    \"\"\"function that fixes defective URLs in the dataframe by adding the release year at the end of the link and separating the links\n",
    "    still defective for further investigation. There are two situations in which we add the year: 1) the link itself is defective since\n",
    "    we retrieve nothing for movie data or 2)the release year on the movie page does not match the official release year of the film\"\"\"\n",
    "    \n",
    "    for row_index, row in df.iterrows(): \n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        page_text = requests.get(link, headers=myheaders).text\n",
    "        soup = BeautifulSoup(page_text,'lxml')\n",
    "        movie_characteristics = soup.find('div', class_= \"panel-body content_body\") \n",
    "        movie_release_year= soup.find('p', class_= \"info\", attrs={'data-qa':'score-panel-subtitle'})\n",
    "        \n",
    "        try:\n",
    "            if(movie_characteristics == None):\n",
    "                new_link = link+'_'+ str(row['Release_Year']) #fixing the link by adding the release year at the end\n",
    "                df.loc[row_index,'Rottentomatoes_URLs'] =  new_link #defining the new link in the data frame\n",
    "                page_text = requests.get(new_link, headers=myheaders).text\n",
    "                soup = BeautifulSoup(page_text,'lxml')\n",
    "                movie_characteristics = soup.find('div', class_= \"panel-body content_body\")\n",
    "                if (movie_characteristics == None): #if some are still not working, we put them in a separate csv file\n",
    "                    with open('links_to_fix.csv', 'a', newline='') as file:\n",
    "                        file.write(f\"{df.loc[row_index,'Movie_Title']}, {new_link},\\n\")\n",
    "                    continue\n",
    "            elif(int(movie_release_year.text[0:4]) != int(row['Release_Year'])): #we must compare integers\n",
    "                new_link = link+'_'+ str(row['Release_Year']) #fixing the link by adding the release year at the end\n",
    "                df.loc[row_index,'Rottentomatoes_URLs'] =  new_link #defining the new link in the data frame\n",
    "                page_text = requests.get(new_link, headers=myheaders).text\n",
    "                soup = BeautifulSoup(page_text,'lxml')\n",
    "                movie_characteristics = soup.find('div', class_= \"panel-body content_body\")\n",
    "                if (movie_characteristics == None): #if some are still not working, we put them in a separate csv file\n",
    "                    with open('links_to_fix.csv', 'a', newline='') as file:\n",
    "                        file.write(f\"{df.loc[row_index,'Movie_Title']}, {new_link},\\n\") \n",
    "                    continue\n",
    "        except ValueError: \n",
    "            new_link = link+'_'+ str(row['Release_Year']) #fixing the link by adding the release year at the end\n",
    "            df.loc[row_index,'Rottentomatoes_URLs'] =  new_link #defining the new link in the data frame\n",
    "            page_text = requests.get(new_link, headers=myheaders).text\n",
    "            soup = BeautifulSoup(page_text,'lxml')\n",
    "            movie_characteristics = soup.find('div', class_= \"panel-body content_body\")\n",
    "            if (movie_characteristics == None): #if some are still not working, we put them in a separate csv file\n",
    "                with open('links_to_fix.csv', 'a', newline='') as file:\n",
    "                    file.write(f\"{df.loc[row_index,'Movie_Title']}, {new_link},\\n\") \n",
    "                continue\n",
    "\n",
    "fix_urls(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d42ed9-6a87-4d2d-b747-69874285259a",
   "metadata": {},
   "source": [
    "After examining the csv file containing all the defective links and manually searching for them in rottentomatoes, some adjustments were made and additional exceptions were added in the movie links generator function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6b786-259a-474a-a76f-274126d6a244",
   "metadata": {},
   "source": [
    "Here below you can see the result.\n",
    "I made **two links purposefully erronous** to demonstrate that the function works properly: \n",
    "\n",
    "1) **Barbie movie** : there is just an error message on the page. The function checks that it cannot scrape any movie information. It appends the release date at the end of the link. It checks the new link and sees that it did not work. It adds the movie to the 'links_to_fix.csv'\n",
    "\n",
    "2) **Lion King movie**: the link works, but it shows the animated movie instead of the live-action remake from 2019. The function compares the release date in the dataframe to the one on the page, sees that they do not match and appends the release date at the end of the link.  It checks the new link and sees that it did not work (since we are on web.archive.org). It adds the movie to the 'links_to_fix.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a1ced-0b4a-48d1-90c6-7f377dcf96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15fa8c-ec99-4104-a2d3-c77c398bb28d",
   "metadata": {},
   "source": [
    "### 2.3 Removing some movies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01667258-552f-4e79-a9a4-11aa6c363553",
   "metadata": {},
   "source": [
    "In our original data frame containing all movie titles and dates, 10 movies were not found on the website and are removed from the dataframe so that the functions we will use later on run smoothly and the sources of errors are easily identified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78524609-8990-42e9-b6ae-cbeaf074c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 rows are removed from the dataframe\n",
    "df_fixed = pd.read_csv(\"../Code/Data/df_with_most_links_fixed.csv\")\n",
    "df_fixed = df_fixed.drop('Unnamed: 0', axis=1)\n",
    "df_links = pd.read_csv(\"../Code/Data/links_to_fix_original.csv\", header=None)\n",
    "df_links.columns =['Movie_Title', 'Rottentomatoes_URLs']\n",
    "df = pd.merge(df_links, df_fixed, on=['Movie_Title'], how='outer', indicator=True).query(\"_merge != 'both'\").drop('_merge', axis=1).reset_index(drop=True)\n",
    "df = df.drop('Rottentomatoes_URLs_x', axis=1)\n",
    "df = df.rename(columns={'Rottentomatoes_URLs_y': 'Rottentomatoes_URLs'})\n",
    "df.to_csv('../Code/Data/df_links_all_ok_new.csv', sep=',',index=True, encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf768572-d64d-47c3-884d-37218f292148",
   "metadata": {},
   "source": [
    "## 3. Scraping Basic Information About The Movies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa026f5-328f-4f30-9459-f484fa20e4f0",
   "metadata": {},
   "source": [
    "Now that we have our list of URLs, we will access each one and scrape the following information that is available on the page:\n",
    "- \"Tomatometer\" score (the critics' average score)\n",
    "- Critics Consensus (the critics' general assessment)\n",
    "- Audience Score (the audience's average score)\n",
    "- Synopsis\n",
    "- Full cast\n",
    "- Rating\n",
    "- Genre\n",
    "- Original Language\n",
    "- Director\n",
    "- Producer\n",
    "- Writer\n",
    "- Release Date (Theaters)\n",
    "- Release Date (Streaming)\n",
    "- Box Office (Gross USA)\n",
    "- Runtime\n",
    "- Distributor\n",
    "- Production Co\n",
    "- Sound Mix\n",
    "- Etc.\n",
    "\n",
    "Most of this data will also be queried by Student C in case the scraped data needs to be completed or corrected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa9e43-155e-47f0-93ff-eff03b1acb26",
   "metadata": {},
   "source": [
    "Basic characteristics are stored in a single class 'panel-body content_body' and therefore have to not only be extracted but parsed as well. \n",
    "\n",
    "For the cast and crew's names and their actual roles in the movie production, the process was more complicated. They were in separate places and needed to be placed in two separate lists before being brought together in tuples by using zip()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b44a4-4c95-496a-a2cc-acc73c5ddd3a",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE** : As of 18.04.24 the below will no longer work with the URLs we previously defined since Rotten Tomatoes updated the movies' pages. \n",
    "As a consequence, we will be using a handful of links gathered from https://web.archive.org/ to demonstrate how the code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2706203d-ac26-42d6-b098-cb92445c12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= pd.read_csv(\"df_links_all_ok.csv\")  <--- this is our data set with all the Oscar movies and their links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3465faf-fbb3-4b1d-9a66-a7c8375a9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"../Code/Data/df_waybackmachine_links.csv\") # <---- this is the csv we will be using to show the code works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e050e8-461a-4dc8-9c40-258db9614141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining additional columns in the data frame\n",
    "basic_characteristics = ['Rating','Genre', 'Original Language', 'Director', 'Producer','Cast and Crew',\n",
    "                         'Writer', 'Release Date (Theaters)', 'Release Date (Streaming)', 'Rerelease Date (Theaters)', 'Box Office (Gross USA)',\n",
    "                         'Runtime', 'Distributor','Production Co', 'Sound Mix', 'Aspect Ratio', 'Synopsis', 'Tomatometer', 'Tomatometer State', \n",
    "                         'Audience Score', 'Critics Concensus']\n",
    "new_vals = [None for item in basic_characteristics]\n",
    "new_col = dict(zip(basic_characteristics, new_vals))\n",
    "df = df.assign(**new_col)\n",
    "#df.set_index('Movie_Title', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23671a86-4d46-4989-99e9-af1e6c022b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user agent info to distinguish our scraper from a malicious bot\n",
    "myheaders = {'User-Agent' : 'Mozilla/5.0 (X11; Linux x86_64) \\ '\n",
    "              'AppleWebKit/537.36 (KHTML, like Gecko) \\ ' \t\t        \n",
    "              'Chrome/84.0.4147.89 Safari/537.36'}\n",
    "\n",
    "headers = {'User-Agent': myheaders}\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None) #to remove some warnings when extracting the crew information \n",
    "\n",
    "#Scraping each movie page's information using Beautiful Soup and Requests\n",
    "def scrape_page_information (df) -> pd.DataFrame:\n",
    "    \"\"\"function to scrape the information on the movie's page and place it in a dataframe\"\"\"\n",
    "    \n",
    "    not_crew_members_list =[]#this list is created for the cleaning section\n",
    "    \n",
    "    for row_index, row in df.iterrows():  # we index the rows in the dataframe so that we can easily insert the scraped data where needed\n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        page_text = requests.get(link, headers=myheaders, timeout=10).text  ###########\n",
    "        soup = BeautifulSoup(page_text,'lxml')\n",
    "        \n",
    "        #We look for the most basic information first (director, producer, runtime, etc.)\n",
    "        try:\n",
    "            movie_characteristics = soup.find('div', class_= \"panel-body content_body\") # in this class we retrieve the basic information\n",
    "            movie_item_labels = list(movie_characteristics.find_all('b', attrs={\"data-qa\":\"movie-info-item-label\"}))\n",
    "            movie_item_values = movie_characteristics.find_all('span', attrs={\"data-qa\":\"movie-info-item-value\"})\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        parsed_data = {}\n",
    "        for idx,label in enumerate(movie_item_labels):\n",
    "            label = label.text.rstrip(':')\n",
    "            lsch = ['\\n', '  '] #a lot of extra spaces and apostrophes to remove which prevent correct parsing\n",
    "            parsed_data[label] = movie_item_values[idx].text\n",
    "            for ch in lsch:\n",
    "                parsed_data[label] = parsed_data[label].replace(ch, '')            \n",
    "            df.loc[row_index, label] = parsed_data[label] # Now we insert the data into the data frame (this is where iterrows is useful)\n",
    "        \n",
    "        #We look for the audiencescore, tomatometerscore, tomatometerstate: \n",
    "        attrs = soup.find('score-board-deprecated').attrs #this is a dictionary from which we retrieve values                 \n",
    "        for attr in ['audiencescore', 'tomatometerscore', 'tomatometerstate']:\n",
    "            df.loc[row_index, 'Tomatometer'] = attrs['tomatometerscore']\n",
    "            df.loc[row_index, 'Tomatometer State'] = attrs['tomatometerstate']\n",
    "            df.loc[row_index, 'Audience Score'] = attrs['audiencescore']\n",
    "\n",
    "        #We look for the critics consensus \n",
    "        consensus = soup.find('span', attrs={'data-qa':'critics-consensus'}) \n",
    "        try:\n",
    "            consensus = consensus.text\n",
    "            df.loc[row_index, 'Critics Concensus'] = consensus\n",
    "        except AttributeError:\n",
    "            df.loc[row_index, 'Critics Concensus'] = 'NaN'\n",
    "            \n",
    "        #We look for the synopsis of the movie   \n",
    "        synopsis = soup.find('p', attrs={'data-qa':'movie-info-synopsis'})\n",
    "        try:\n",
    "            synopsis = synopsis.text\n",
    "            df.loc[row_index, 'Synopsis'] = synopsis\n",
    "        except AttributeError:\n",
    "            df.loc[row_index, 'Synopsis'] = 'NaN'\n",
    "\n",
    "        #We look for all the crew members and their roles/jobs in the movie production \n",
    "        crew = soup.css.select('a > p', class_='p--small', attrs={'data-qa':'cast-crew-item-link', 'id':'cast-and-crew', 'href':True})\n",
    "        role = soup.find_all('p', class_='p--small')\n",
    "        \n",
    "        roles_list=[]\n",
    "        for r in role:\n",
    "            roles_list.append(r.get_text(strip=True))\n",
    "               \n",
    "        crew_list=[]\n",
    "        for c in crew[1:]:\n",
    "            crew_list.append(c.get_text(strip=True))\n",
    "\n",
    "        not_crew_members = crew_list[len(roles_list):]#We append this to 'not_crew_members_list' created for the cleaning section \n",
    "        for n in not_crew_members:\n",
    "            not_crew_members_list.append(n)\n",
    "        \n",
    "        crew_list = crew_list[:len(roles_list)] #we have names in crew_list that are not actual crew members, \n",
    "                                                #so we select only the ones that correspond to each role\n",
    "       \n",
    "        tuples = zip(crew_list, roles_list) #we make a tuple. A dictionary cannot be used because some people have several jobs.  \n",
    "        full_crew = [f'{pair[0]} ({pair[1]})' for pair in tuples] #we iterate over the tuple to format it as we wish\n",
    "        full_crew = ', '.join(map(str, full_crew))\n",
    "        \n",
    "        df['Cast and Crew'] = df['Cast and Crew'].astype('object')\n",
    "        df.at[row_index, 'Cast and Crew'] =full_crew # to populate the rows we cannot use \"loc\" because loc can refer to values \n",
    "                                                              # as well as rows and columns. A ValueError kept appearing as a consequence.   \n",
    "                                                              # 'at' always refers to a single value, which is what we need for a list. \n",
    "    return df, not_crew_members_list\n",
    "\n",
    "\n",
    "df, not_crew_members_list = scrape_page_information(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0f2c8-c883-45ea-b165-0fdbd7244d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d846550-f39a-42df-9c71-1f24253128d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid running the function again, we pickle the set of our not_crew_members_list:\n",
    "\n",
    "not_crew_members_list_set = list(set(not_crew_members_list))\n",
    "\n",
    "with open('not_crew_pickle_extra.pkl', 'wb') as f:\n",
    "    pickle.dump(not_crew_members_list_set,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b56af6-b018-4dbf-a7d7-4620ac8c4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df= pd.read_csv(\"movie_page_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe13ef9-51bb-4ab3-940a-59347120e8cf",
   "metadata": {},
   "source": [
    "## 4. Scraping Critics Reviews of the Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49bd18-f62f-4657-b1f7-14c120e46e82",
   "metadata": {},
   "source": [
    "Now we need to obtain the critics' comments about each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107c809b-5e01-4140-9089-8ed45866b386",
   "metadata": {},
   "source": [
    "To do so, we first define the links we will be accessing. In this case, we simply need to add '/reviews?intcmp=rt-scorecard_tomatometer-reviews' at the end of the URLs we previously defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94723bb-55fd-49c3-9264-50b365eedb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv(\"../Code/Data/df_links_all_ok.csv\")\n",
    "df_links = df_links.drop('Unnamed: 0', axis=1)\n",
    "df_links.insert(1, 'Reviews_URLs', True)\n",
    "for row_index, row in df_links.iterrows():\n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        df_links.loc[row_index, 'Reviews_URLs'] = link+'/reviews?intcmp=rt-scorecard_tomatometer-reviews'\n",
    "links = df_links['Reviews_URLs']\n",
    "df_links = pd.DataFrame(links)\n",
    "df_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b4c22-ce4b-4a9a-b07e-1ba44933cb1e",
   "metadata": {},
   "source": [
    "For scraping the actual critics' reviews, we must load all of them. This means that we must first press the \"load more\" button until there is no longer a button to press. \n",
    "Then, we should scrape the needed data : \n",
    "\n",
    "- Name of the critic\n",
    "- Name of the journal for which he writes\n",
    "- Review content\n",
    "- Whether the review is negative or positive ('rotten' or 'fresh')\n",
    "- The date the review was posted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffc679-cc53-4ce5-9dc6-0fc53c8bff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping all of the critics' comments using Selenium \n",
    "\n",
    "#we define our headers for the selenium driver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "myheaders = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'\n",
    "chrome_options.add_argument(f\"user-agent={myheaders}\")\n",
    "#chrome_options.add_argument('--headless=new')  \n",
    "headers = {'User-Agent': myheaders}\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def scrape_critics_reviews (df_links, driver) -> pd.DataFrame:\n",
    "    \"\"\"function to scrape all the critics reviews by using Selenium and placing them in data frames which we then concatenate into a single\n",
    "    dataframe\"\"\"\n",
    "    \n",
    "    data_frames_list=[] #this is where we store the data frames\n",
    "    \n",
    "    for row_index, row in df_links.iterrows():\n",
    "        link = row['Reviews_URLs']\n",
    "        driver.get(link)\n",
    "        \n",
    "        #keep clicking the 'load more' button as long as it exists to get all the comments \n",
    "        while True:\n",
    "            time.sleep(1.5)\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"reviews\"]/div[2]/rt-button').click()\n",
    "            except (NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException):   #when the button disappears, we need to 'break' the loop\n",
    "                break \n",
    "\n",
    "        #scrape each comment's author, the journal they write for, the content of the comment and also if they were 'fresh'(positive) \n",
    "        #or 'rotten' (negative)\n",
    "        reviews = driver.find_elements(By.CLASS_NAME,'review-row')\n",
    "        reviews_list = []\n",
    "    \n",
    "        for review in reviews:\n",
    "            try:\n",
    "                name_critic = review.find_element(By.CSS_SELECTOR,'a.display-name').text #the x-path showed almost no IDs and instead a lot of tags so it is easier to select elements by CSS_SELECTOR\n",
    "                name_journal = review.find_element(By.CSS_SELECTOR,'a.publication').text\n",
    "                comment = review.find_element(By.CSS_SELECTOR,'p.review-text').text\n",
    "                positive_or_negative = review.find_element(By.CSS_SELECTOR,'score-icon-critic-deprecated[alignment=left]').get_attribute('state')\n",
    "                date = review.find_element(By.CSS_SELECTOR,'span[data-qa=review-date]').text\n",
    "            except (NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException):\n",
    "                continue\n",
    "            \n",
    "            reviews_list.append({\n",
    "                'Date':date, 'Author':name_critic, 'Journal':name_journal, 'Positive_Negative':positive_or_negative, 'Reviews':comment})\n",
    "            \n",
    "            df_temp = pd.DataFrame(reviews_list)\n",
    "            df_temp.insert(1, 'Reviews_URLs', row['Reviews_URLs'])\n",
    "            \n",
    "         \n",
    "        data_frames_list.append(df_temp)\n",
    "\n",
    "    driver.quit() #once done we close the driver\n",
    "    df_result = pd.concat(data_frames_list, ignore_index=True) #once we are done, we concatenate all the data frames together\n",
    "    return df_result\n",
    "\n",
    "df_result = scrape_critics_reviews(df_links, driver)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b195fc74-6bb5-4335-af94-39d1d314289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_result.to_csv('data_all_reviews.csv', sep=',',index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a08cec-15b5-42be-a983-c52c173df890",
   "metadata": {},
   "source": [
    "## 5. Merge the Datasets - Movie Characterisitcs and Critics' Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54560c00-1deb-4e7b-b155-0ff902ee4021",
   "metadata": {},
   "source": [
    "Finally, we merge the datasets for the movie page information and the dataset for the reviews. The result is the stage 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1cec3f-0024-44df-8354-b6cd3c73c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.read_csv(\"../Code/Data/data_all_reviews.csv\")\n",
    "df_moviedata = pd.read_csv(\"../Code/Data/movie_page_data.csv\")\n",
    "df_moviedata = df_moviedata.drop(['Unnamed: 0','Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c33e9-1cdb-4ef7-9bd6-105d0c891552",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_characteristics = ['Reviews_URLs']\n",
    "reviews_vals = [None for item in reviews_characteristics]\n",
    "new_col = dict(zip(reviews_characteristics, reviews_vals))\n",
    "df_moviedata = df_moviedata.assign(**new_col)\n",
    "for row_index, row in df_moviedata.iterrows():\n",
    "        link = row['Rottentomatoes_URLs']\n",
    "        df_moviedata.loc[row_index, 'Reviews_URLs'] = link+'/reviews?intcmp=rt-scorecard_tomatometer-reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9f5a5-ebcd-463c-9320-1e162977f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we want to keep the movies that also do not have any reviews, we merge with the \"left\" method (on the movie characteristics data set) \n",
    "\n",
    "df_merged = pd.merge(df_moviedata, df_result, on='Reviews_URLs', how='left')\n",
    "df_merged.to_csv('Vedenikova_Vitalia_studentB_stage1.csv', sep=',',index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c27e41-e3a3-490f-bff2-fa12cebec27b",
   "metadata": {},
   "source": [
    "## 6. Clean the Resulting Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59708f-0506-4e7c-8011-2017662d3ec6",
   "metadata": {},
   "source": [
    "### 6.1 Encode the CSV file correctly\n",
    "Some accented letters or other language specific symbols had not been properly encoded into the csv file (ex: \"Régional\" was \"RÃ©gional\").\n",
    "We simply encode the file as 'utf-8-sig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd2f5e-fd62-499b-bc77-d8209c167835",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Vedenikova_Vitalia_studentB_stage1.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b219d8-898f-4337-85cd-9da1f6a1be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vedenikova_Vitalia_studentB_stage3.csv', sep=',',index=False, encoding='utf-8-sig')\n",
    "df = pd.read_csv('Vedenikova_Vitalia_studentB_stage3.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2905e25-66b2-443d-9663-f718989b1ed7",
   "metadata": {},
   "source": [
    "### 6.2 Remove Columns containing unnecessary data and give standardized names to columns\n",
    "We should remove:\n",
    "- columns we created to generate links\n",
    "- columns containing data we scraped just because it was contained in the same class as actually useful information.\n",
    "\n",
    "We also standardize the columns' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411ab4cc-f9f6-4821-be4e-12483b7ec7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0','Unnamed: 0.1','URL_Movie_Names', 'Reviews_URLs', 'Release_Year',\n",
    "              'Rerelease Date (Theaters)', 'Release Date (Streaming)', 'Release Date (DVD)','Sound Mix', 'Aspect Ratio', \n",
    "              'View the collection'], axis=1)\n",
    "\n",
    "df = df.rename(columns={'Date':'Review_Date', 'Original Language':'Original_Language', 'Cast and Crew': 'Cast_And_Crew', \n",
    "                        'Release Date (Theaters)': 'Release_Date_(Theaters)', 'Box Office (Gross USA)':'Box_Office_(Gross_USA)', \n",
    "                        'Production Co':'Production_Co','Tomatometer State':'Tomatometer_State', \n",
    "                        'Audience Score':'Audience_Score','Critics Concensus':'Critics_Concensus'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e665b8-a8de-4f3f-bc3a-2bc12609f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result:\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf55c96-cbd4-448e-9776-e215882cd6e9",
   "metadata": {},
   "source": [
    "### 6.3 Remove the extra spaces (such as \\n, \\s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe4ab3-9dcc-4119-84b3-e1795b5321da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of extra spaces we want to remove:\n",
    "df['Cast_And_Crew'][2594]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0106c6-8e76-4a9f-ab96-fd4e2e1cee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove leading and trailing spaces\n",
    "col_list = ['Synopsis','Cast_And_Crew']\n",
    "df[col_list] = df[col_list].apply(lambda x : x.str.strip())\n",
    "\n",
    "#Remove two or more spaces \n",
    "df = df.replace(r'\\n{1,}\\s{2,}', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f318cc-7f8c-49e6-b413-c3c97c829562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result:\n",
    "df['Cast_And_Crew'][2594]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb2e4e-faa1-4659-9fa0-f7c520c08581",
   "metadata": {},
   "source": [
    "### 6.4 Remove Unnecessary Characters and Inaccurate Information for the Crew\n",
    "- We remove the \"limited\" in dates of the the Release_Date_(Theater) column.\n",
    "- In the Cast_And_Crew column, we had rare cases of not scraping persons' names, but names of objects of the same HTML class (ex: Awards Leaderboard: Top Movies of 2022 (Original Music)). These should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6e91f-f711-4e4a-aa66-86a69c43f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the trailing characters after the release dates:\n",
    "df['Release_Date_(Theaters)'] = df['Release_Date_(Theaters)'].str[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810bd2d-a487-40ad-a9b5-2ccbfa7dcc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove innacurate information in the Cast_And_Crew column, we use the \"not_crew_members_list\" which we defined \n",
    "#while running our function \"scrape_page_information\" :\n",
    "\n",
    "#We deserialize our variable:\n",
    "with open('../Code/Data/not_crew_pickle_extra.pkl', 'rb') as f:\n",
    "    not_crew_list = pickle.load(f) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54b33d-d819-42ae-ae4e-7d667aa995c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of extra elements with'Awards Leaderboard: Top Movies of 2022 (Original Music)' at the end of the string:\n",
    "df['Cast_And_Crew'][6915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be474c9c-ae39-4e4b-9242-19d910e1dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove innacurate cast information: \n",
    "for row_index, row in df.iterrows():\n",
    "    for element in not_crew_list:\n",
    "        try: \n",
    "            if element in row['Cast_And_Crew']:\n",
    "                text= row['Cast_And_Crew']\n",
    "                t = re.sub(rf'{element}.*\\)','',text)[0:-2] #removing the elements and slicing to remove the comma at the end of the list \n",
    "                df.loc[row_index, 'Cast_And_Crew'] = t\n",
    "        except TypeError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f59173-4356-46cf-98cf-62ee7a916aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n",
    "df['Cast_And_Crew'][6915]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9cc1c7-227c-447d-92c8-67c48357533c",
   "metadata": {},
   "source": [
    "### 6.5 Handle Missing Values\n",
    "\n",
    "- In the Cast_And_Crew column, there might be some empty brackets when the role of the crew member is not indicated in Rotten Tomatoes. These empty brackets should contain \"NaN\".\n",
    "- A lot of information is missing for certain movies (ex: no rottentomatoes score, no box office revenue, no distributor, etc.). In these cases, we should make sure that the empty boxes contain \"NaN\" (even in the csv file) so as to be clearly identified. \n",
    "- If one of the rows is missing a review, it should be removed unless the movie it belongs to has in fact no reviews. In this case, it should be kept so that the movie's information is not lost and the movie is further discussed in the group work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ab61e-88ce-4902-888b-3e9d34391e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing NaN between empty brackets in the crew and cast lists :\n",
    "df['Cast_And_Crew'] = df['Cast_And_Crew'].str.replace('()', '(NaN)')\n",
    "\n",
    "# Make sure that all blank fields have NaN: \n",
    "df = df.fillna(\"NaN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5e907c-94c9-4349-9643-462b8ff66b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with 'NaN' under \"Review\" when there is more than one review per movie:\n",
    "\n",
    "# First, count the amount of reviews for each movie and identify all the movies with more than one review:\n",
    "review_counts = df[\"Movie_Title\"].value_counts().sort_index()\n",
    "review_counts = review_counts.to_frame()\n",
    "\n",
    "subcategory = review_counts[review_counts['count']>1]\n",
    "subcategory = subcategory.reset_index()\n",
    "subcategory_list = [movie for movie in subcategory['Movie_Title']]\n",
    "\n",
    "# Then, remove the row where there is no review and the movie is part of our subcategory_list:\n",
    "for row_index, row in df.iterrows():\n",
    "    if df.at[row_index,'Reviews'] == 'NaN' and row['Movie_Title'] in subcategory_list:\n",
    "        df.drop(row_index,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baee8de-f4bc-42b5-8cff-b3d634fccf42",
   "metadata": {},
   "source": [
    "### 6.6 Standardize Data \n",
    "- Make sure the release dates and review dates are all in the same date format\n",
    "- The runtime of the movies should be in minutes (a single time unit)\n",
    "- The Box Office Revenues should be expressed as integers since we will be plotting this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a0444-7a82-447b-8738-717700059c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data (df) -> pd.DataFrame:\n",
    "    '''Standardizes the data in the columns 'Release_Date_(Theaters)','Review_Date', 'Runtime' and 'Box_Office_(Gross_USA)' '''\n",
    "    \n",
    "    # Put the release dates and review dates in the same date format\n",
    "    df[['Release_Date_(Theaters)','Review_Date']] = df[['Release_Date_(Theaters)','Review_Date']].apply(lambda x: pd.to_datetime(x, errors='coerce'))\n",
    "\n",
    "    #Convert the runtime of the movies to minutes\n",
    "    time = df['Runtime'].str.extract('(\\d+)h', expand=False).astype(float) * 60 +  df['Runtime'].str.extract('(\\d+)m', expand=False).astype(float)\n",
    "    df['Runtime'] = time\n",
    "\n",
    "    #Convert the box office revenues into integers\n",
    "    df['Box_Office_(Gross_USA)'] = df['Box_Office_(Gross_USA)'].str.replace('$', '')\n",
    "    df['Box_Office_(Gross_USA)'] = df['Box_Office_(Gross_USA)'].replace({'K': 'e+03', 'M': 'e+06'}, regex=True).astype(float) \n",
    "    \n",
    "    return df\n",
    "\n",
    "df = standardize_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7637685-6570-4766-a567-91da23ea5fa7",
   "metadata": {},
   "source": [
    "### 6.7 Remove Duplicates\n",
    "Some reviews appear twice and should be removed. The ones with 'NaN' should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5311ddf7-d989-4eb7-8da8-6426f9a82e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"NaN\")\n",
    "df = pd.concat([df[df['Reviews']=='NaN'], df[df['Reviews']!='NaN'].drop_duplicates(['Reviews'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ec82e-aa8f-46ba-839b-bc2e825e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We check that we removed all duplicates except the 'NaN':\n",
    "counts = df[\"Reviews\"].value_counts().sort_index()\n",
    "counts = counts.to_frame()\n",
    "reviews_with_duplicates = counts[counts['count']>1]\n",
    "reviews_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d345a-30ee-4cd5-b9b0-9fda10d98414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['Movie_Title','Review_Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f67ad-3610-41c3-94a7-75b971e8a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vedenikova_Vitalia_studentB_stage3.csv', sep=',',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b495da5-cc83-4236-b7f2-396762eebedd",
   "metadata": {},
   "source": [
    "## 7. Enriching the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c6566-1636-423e-8305-a8e90540a548",
   "metadata": {},
   "source": [
    "### 7.1 Identifying Outliers in terms of quantities of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4dde59-cc0b-4a36-902f-a383c7890ddf",
   "metadata": {},
   "source": [
    "Usually, we would take Q3 + 1.5 * IQR as an upper boundary and Q1 - 1.5 * IQR as a lower boundary. In this case, however, the lower boundary is negative. Furthermore, calculating Q3 + 1.5 * QR yields a very large value of 688, whereas the maximum amount of reviews per movie is 600:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b1669-86f7-48fb-a9be-882652af0e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_counts = df[\"Movie_Title\"].value_counts().sort_index()\n",
    "review_counts = review_counts.to_frame()\n",
    "review_counts.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436646e-7e79-400b-a6ba-1d4517720fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7371391a-6640-4926-8b34-fb5367f48f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = review_counts['count'].quantile(0.25)\n",
    "Q3 = review_counts['count'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(f'This is the lower boundary : {Q1-1.5*IQR}\\nThis is the upper boundary: {Q3+1.5*IQR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaf062b-e9bc-4fb3-a2e0-37383bfebfcc",
   "metadata": {},
   "source": [
    "Looking at the below boxplot, it makes sense to define the lower_boundary as 0. We also take the upper_boundary as 2 standard deviations above the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec4565-8091-4fdb-9169-df3177023571",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = review_counts.boxplot(column=['count'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be210a5d-4c3b-4cc4-9214-e155deed17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_boundary = review_counts['count'].mean() + 2*review_counts['count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd2c63-d7ea-4fdc-a47e-c6e788b80fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_counts['count'] = review_counts['count'] > upper_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c97eeb-13dc-4929-9f05-07f984751c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(review_counts,how='left', on='Movie_Title')\n",
    "df.rename(columns={'count': 'Outlier_Reviews_Amount_(True/False)'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359412ad-45cc-4b20-a9ef-546eda576462",
   "metadata": {},
   "source": [
    "### 7.2 Percentage of difference between the critics' overall score and the audience's overall score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941ca1c-5e31-4c14-b9e2-03c5bf1a49b4",
   "metadata": {},
   "source": [
    "This could help provide an interesting insight into how much the audience's appreciation of the movie coincides with that of critics'. Some examples of questions : are critics on average more appreciative of Oscars movies than audiences? why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61788655-6d99-41b1-92da-2e412c341165",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None \n",
    "df['Difference_Critics_Audience'] = df['Tomatometer'] - df['Audience_Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280de40-4339-4a89-896f-6d45fbab9b7c",
   "metadata": {},
   "source": [
    "### 7.3 Movies' box office revenue adjusted for inflation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f86d5-5df2-4d06-be17-d4256e305dcf",
   "metadata": {},
   "source": [
    "These adjusted amounts brought forward to 2024 will allow us to accurately compare them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12152069-1dac-4002-be92-82cd68dc0d0d",
   "metadata": {},
   "source": [
    "All box office revenue is listed in USD on Rotten Tomatoes. We first download the CPI from the FED's website: https://fred.stlouisfed.org/series/CPIAUCNS and from there adjust the box office amounts to today's value. The file has been provided in the code folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb56da4-a803-4b90-a948-59422171e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inflation = pd.read_csv('CPI_data.csv')\n",
    "df_inflation.rename(columns={'CUUS0000SA0': 'CPI'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fccfb-276e-4ab8-8a69-544fe823d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust the dates' format\n",
    "df_inflation['DATE'].str.split('/')\n",
    "df_inflation['DATE'] = df_inflation['DATE'].apply(lambda x : x[0:4])\n",
    "df_inflation.drop_duplicates(subset='DATE', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbf9c6-2560-4793-aa0a-c8162b2309d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = df['Release_Date_(Theaters)'].str[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd005734-7b81-4032-ba4f-116f1aa700e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPI_Multiplier defined with reference year being 2023 (we take the same value for 2024 for simplicity's sake)\n",
    "df_inflation['Multiplier'] = df_inflation['CPI'].iloc[-1] / df_inflation['CPI']\n",
    "df_inflation.loc[29] = [int(2024), 302.408, 1.000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d0ff68-893a-4c65-b43c-d276133761e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_inflation, how='left', on='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9709f68-f66f-4cb6-9e67-350018ac2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Box_Office_Adjusted_Inflation_(USD)'] = df['Box_Office_(Gross_USA)']*df['Multiplier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25472fc0-3af2-410b-b3c7-8b6d871f0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, we make some minor adjustments before exporting to csv:\n",
    "df.fillna(0, inplace=True)\n",
    "df.drop(['Multiplier','CPI','DATE'], axis=1, inplace=True)\n",
    "\n",
    "df = df[['Movie_Title', 'Release_Date_(Theaters)','Rottentomatoes_URLs', 'Rating', 'Genre', 'Original_Language', 'Director', \n",
    "             'Producer','Writer', 'Cast_And_Crew', 'Distributor','Production_Co', 'Synopsis','Runtime', 'Box_Office_(Gross_USA)', \n",
    "             'Box_Office_Adjusted_Inflation_(USD)','Audience_Score', 'Tomatometer_State', 'Tomatometer', 'Critics_Concensus',\n",
    "            'Difference_Critics_Audience','Review_Date','Author', 'Journal','Positive_Negative','Reviews',\n",
    "             'Outlier_Reviews_Amount_(True/False)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95d5a1-ddfc-4718-9ba4-f27bc8b98073",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "df['Box_Office_Adjusted_Inflation_(USD)'] = df['Box_Office_Adjusted_Inflation_(USD)'].fillna(0).astype(int)\n",
    "df['Box_Office_Adjusted_Inflation_(USD)'].replace(0,np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbff1c7-0c69-4d2e-9f4a-5c9c8ccbd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Vedenikova_Vitalia_studentB_stage3.csv', sep=',',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c00ef77-b7f1-4c7b-8999-b15a735a8325",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61699d84-9585-492f-9f9c-699d092ca4b8",
   "metadata": {},
   "source": [
    "Finally, we obtain a data frame where we have a row for each review. The left-most rows contain all the basic characteristics of the movies (release date, director, producer, etc.), and the right-most rows contain the reviews and related columns. \n",
    "\n",
    "We decided to keep the data frame in this format on purpose so that it is easier for Student C to manipulate as needed for the final merge. Indeed, they will not have to think about ways to reset indexes if the information is presented in such a straightforward way. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
